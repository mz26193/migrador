# pega esto en una celda de Jupyter
import re
from pathlib import Path
import json
from datetime import timedelta

# ---------------- CONFIGURACIÓN (ajusta si quieres) ----------------
FUNC_MAP = {
    r'\btoday\s*\(\s*\)': 'current_date',
    r'\bdate\s*\(\s*\)': 'current_date',
    r'\bdatetime\s*\(\s*\)': 'current_timestamp',
    r'\bsubstr\s*\(': 'substring(',
    r'\bupcase\s*\(': 'upper(',
    r'\blowcase\s*\(': 'lower(',
    r'\btrim\s*\(': 'trim(',
    r'\bstrip\s*\(': 'trim(',
    r'\bcatx\s*\(': 'concat_ws(',
    r'\bcompress\s*\(': 'regexp_replace(',   # heurístico
    r'\bindex\s*\(': 'position(',
    r'\bintnx\s*\(': "/* intnx(...) -> revisar: usar date_trunc/interval en PG */",
    r'\bintck\s*\(': "/* intck(...) -> revisar: usar age/EXTRACT o generate_series */",
    r'\bput\s*\(': "/* put(...) -> revisar to_char en PG */",
    r'\binput\s*\(': "/* input(...) -> revisar to_date / cast en PG */",
    r'\bcats?\s*\(': 'concat(',
    r'\bcoalescec\s*\(': 'coalesce(',
    r'\bmean\s*\(': 'avg(',
    r'\bround\s*\(': 'round(',
}

# Regexes
RE_PERCENT_LET = re.compile(r'^\s*%let\s+([A-Za-z_]\w*)\s*=\s*(.+?);', re.IGNORECASE)
RE_LIBNAME = re.compile(r'^\s*libname\s+([A-Za-z_]\w*)\s+["\']?(.+?)["\']?\s*;', re.IGNORECASE)
RE_INCLUDE = re.compile(r'^\s*%include\s+["\']?(.+?)["\']?\s*;', re.IGNORECASE)
RE_MACRO_START = re.compile(r'^\s*%macro\s+([A-Za-z_]\w*)', re.IGNORECASE)
RE_MACRO_END = re.compile(r'^\s*%mend\b', re.IGNORECASE)
RE_PROC_SQL_START = re.compile(r'^\s*proc\s+sql\b', re.IGNORECASE)
RE_QUIT = re.compile(r'^\s*quit\s*;', re.IGNORECASE)
RE_RUN = re.compile(r'^\s*run\s*;', re.IGNORECASE)
RE_DATA_START = re.compile(r'^\s*data\s+([A-Za-z0-9_$.]+)\s*;', re.IGNORECASE)
RE_SET = re.compile(r'^\s*set\s+([A-Za-z0-9_$.]+)\s*;', re.IGNORECASE)

# patrón para encontrar FROM/JOIN/CREATE/INSERT/INTO, permitimos &var o nombres con puntos y guiones bajos
RE_SQL_TABLE = re.compile(r'\b(?:from|join|into|update|delete\s+from|create\s+table)\s+([A-Za-z0-9_.&]+)', re.IGNORECASE)
RE_CREATE_TABLE = re.compile(r'\bcreate\s+table\s+([A-Za-z0-9_.&]+)', re.IGNORECASE)

# ---------------- Utils ----------------
def apply_func_map(text: str) -> str:
    out = text
    for patt, repl in FUNC_MAP.items():
        out = re.sub(patt, repl, out, flags=re.IGNORECASE)
    return out

def resolve_macros_in_text(text: str, macros: dict) -> str:
    # Resuelve &&var y &var de forma heurística
    t = text
    # primera pasada: reemplaza &&name
    t = re.sub(r'&&([A-Za-z_]\w*)', lambda m: macros.get(m.group(1), ''), t)
    # luego &name
    t = re.sub(r'&([A-Za-z_]\w*)', lambda m: macros.get(m.group(1), ''), t)
    return t

def normalize_table_name(name: str, macros: dict):
    # Resuelve macros y quita puntos dobles lib..table -> lib.table
    if not name:
        return name
    resolved = resolve_macros_in_text(name, macros)
    resolved = resolved.replace('..', '.')
    # quitar comillas posibles
    return resolved.strip().strip("'\"")

# ---------------- Translator clase ----------------
class SAS2PG_Jupyter:
    def __init__(self, text, base_path=None):
        self.base_path = Path(base_path) if base_path else Path('.')
        self.original_lines = text.splitlines()
        self.lines = list(self.original_lines)  # trabajaremos sobre esto
        self.macros = {}        # %let variables
        self.libraries = {}     # libname -> ruta
        self.macro_defs = {}    # macros simples guardadas
        self.inputs = set()
        self.outputs = set()
        self.blocks = []        # bloques detectados traducidos o sugeridos
        self.warnings = []
        # contadores
        self.total_lines = len(self.lines)
        self.proc_sql_count = 0
        self.data_count = 0
        self.complex_data_count = 0

    def preprocess(self):
        # extraer %let y libname y procesar %include si es local
        new_lines = []
        i = 0
        while i < len(self.lines):
            ln = self.lines[i]
            m_let = RE_PERCENT_LET.match(ln)
            if m_let:
                name = m_let.group(1)
                val = m_let.group(2).strip()
                if (val.startswith("'") and val.endswith("'")) or (val.startswith('"') and val.endswith('"')):
                    val = val[1:-1]
                self.macros[name] = val
                i += 1
                continue
            m_lib = RE_LIBNAME.match(ln)
            if m_lib:
                lib = m_lib.group(1)
                path = m_lib.group(2)
                self.libraries[lib] = path
                i += 1
                continue
            m_inc = RE_INCLUDE.match(ln)
            if m_inc:
                inc = m_inc.group(1)
                p = (self.base_path / inc).resolve()
                if p.exists():
                    inc_text = p.read_text(encoding='utf-8', errors='ignore').splitlines()
                    # insertar líneas del include
                    self.lines[i:i+1] = inc_text
                    # no incrementar i para procesar el include
                    continue
                else:
                    self.warnings.append(f"%include no encontrado: {inc}")
                    i += 1
                    continue
            m_macro = RE_MACRO_START.match(ln)
            if m_macro:
                name = m_macro.group(1)
                body = []
                i += 1
                while i < len(self.lines) and not RE_MACRO_END.match(self.lines[i]):
                    body.append(self.lines[i])
                    i += 1
                # saltar %mend si existe
                if i < len(self.lines) and RE_MACRO_END.match(self.lines[i]):
                    i += 1
                self.macro_defs[name] = "\n".join(body)
                # si macro parece generar SQL o DATA, avisar
                if any(re.search(r'\b(proc sql|data\b)', l, re.IGNORECASE) for l in body):
                    self.warnings.append(f"Macro %{name} definida; puede contener lógica compleja (revisar).")
                continue
            # si no fue %let/lib/include/macro guardamos la línea
            new_lines.append(ln)
            i += 1
        self.lines = new_lines

    def translate(self):
        i = 0
        n = len(self.lines)
        while i < n:
            ln = self.lines[i]
            # detectar proc sql (incluye variantes)
            if RE_PROC_SQL_START.match(ln):
                i = self._handle_proc_sql(i)
                continue
            # detectar data step
            if RE_DATA_START.match(ln):
                i = self._handle_data_step(i)
                continue
            i += 1

    def _handle_proc_sql(self, start):
        # captura el bloque hasta quit;
        buf = []
        i = start + 1
        self.proc_sql_count += 1
        while i < len(self.lines):
            if RE_QUIT.match(self.lines[i]):
                break
            buf.append(self.lines[i])
            i += 1
        raw = "\n".join(buf)
        if not raw.strip():
            self.warnings.append(f"PROC SQL vacío cerca de línea {start+1}")
            return i+1
        # resolver macros en el bloque
        resolved = resolve_macros_in_text(raw, self.macros)
        # aplicar map de funciones
        mapped = apply_func_map(resolved)
        # detectar tablas (FROM/JOIN/CREATE/INTO)
        for m in RE_SQL_TABLE.findall(resolved):
            t = normalize_table_name(m, self.macros)
            if t:
                # si la palabra es create table ... la tratamos como output
                if re.search(r'\bcreate\s+table\b', m, re.IGNORECASE):
                    self.outputs.add(t)
                else:
                    # no siempre create -> usamos heurístico: si dentro del bloque aparece create table con nombre distinto, marcar ambos
                    self.inputs.add(t)
        # detectar create table usando regex aparte (captura name)
        for m in RE_CREATE_TABLE.findall(resolved):
            nm = normalize_table_name(m, self.macros)
            if nm:
                self.outputs.add(nm)

        # detectar connect/disconnect y conservar en comentarios
        conn_lines = []
        for ln in buf:
            if re.search(r'\bconnect to\b', ln, re.IGNORECASE):
                conn_lines.append(ln.strip())
            if re.search(r'\bdisconnect from\b', ln, re.IGNORECASE):
                conn_lines.append(ln.strip())

        comment = f"-- PROC SQL (líneas {start+1}-{i+1})\n"
        if conn_lines:
            comment += "-- Conexiones detectadas:\n" + "\n".join("-- " + c for c in conn_lines) + "\n"
        comment += "-- SQL traducido (heurístico SAS->Postgres):\n"
        comment += mapped + "\n"
        # añadir el bloque
        self.blocks.append({
            'type': 'proc_sql',
            'start': start+1,
            'end': i+1,
            'sas': raw,
            'pg': mapped,
            'comment': comment
        })
        return i+1

    def _handle_data_step(self, start):
        m = RE_DATA_START.match(self.lines[start])
        dest = m.group(1)
        self.data_count += 1
        i = start + 1
        body = []
        set_table = None
        loop = False
        has_fileio = False
        while i < len(self.lines):
            ln = self.lines[i]
            body.append(ln)
            setm = RE_SET.match(ln)
            if setm:
                set_table = normalize_table_name(setm.group(1), self.macros)
                if set_table:
                    self.inputs.add(set_table)
            if re.search(r'\bdo\b', ln, re.IGNORECASE):
                loop = True
            if re.search(r'\bfile\b|\binput\b', ln, re.IGNORECASE):
                has_fileio = True
            if RE_RUN.match(ln):
                break
            i += 1
        # registrar salida
        dest_norm = normalize_table_name(dest, self.macros)
        if dest_norm:
            self.outputs.add(dest_norm)

        # heurística: si solo SET y sin loop -> es copia/simple transformación
        if set_table and not loop and not has_fileio:
            pg = f"CREATE TABLE {dest_norm} AS\nSELECT * FROM {set_table};"
            comment = (f"-- DATA simple (líneas {start+1}-{i+1}): copia/transformación simple\n"
                       f"-- SAS original (resumen): DATA {dest}; SET {set_table}; ... RUN;\n"
                       f"{pg}\n"
                       f"-- Sugerencia Python/pandas:\n"
                       f"import pandas as pd\n"
                       f"df = pd.read_sql('SELECT * FROM {set_table}', con)\n"
                       f"df.to_sql('{dest_norm.split('.')[-1]}', con, schema='{dest_norm.split('.')[0]}' if '.' in '{dest_norm}' else None, if_exists='replace', index=False)\n")
            self.blocks.append({
                'type': 'data_simple',
                'start': start+1,
                'end': i+1,
                'sas': "\n".join(body),
                'pg': pg,
                'comment': comment
            })
        else:
            # DATA complejo: generar sugerencia pandas heurística
            self.complex_data_count += 1
            suggestion = self._pandas_suggestion_for_data(body, dest_norm, set_table, loop, has_fileio)
            comment = (f"-- DATA complejo (líneas {start+1}-{i+1}): requiere conversión manual o revisión\n"
                       f"-- SAS original (extracto):\n"
                       + "\n".join("-- " + l for l in body[:40]) +
                       ("\n-- ... (recortado)\n" if len(body) > 40 else "\n") +
                       f"-- Sugerencia Python/pandas (esqueleto):\n{suggestion}\n")
            self.blocks.append({
                'type': 'data_complex',
                'start': start+1,
                'end': i+1,
                'sas': "\n".join(body),
                'pg': f"/* DATA complejo destino {dest_norm} -> revisar */",
                'comment': comment
            })

        return i+1

    def _pandas_suggestion_for_data(self, body_lines, dest, src, has_loop, has_fileio):
        # Genere un esqueleto de pandas a partir de heurísticos
        lines = []
        lines.append("import pandas as pd")
        if src:
            lines.append(f"# Leer fuente (ejemplo): df = pd.read_sql('SELECT * FROM {src}', con)")
        elif has_fileio:
            lines.append("# Si los datos vienen de archivos planos, usar pd.read_csv / pd.read_fwf")
        else:
            lines.append("# Fuente no identificada explícitamente")
        lines.append("# Aquí aplicar transformaciones detectadas en SAS (IF/THEN, DO loops, merges, retain, lag, etc.)")
        lines.append("# Ejemplos comunes:")
        lines.append("## IF ... THEN ... -> df.loc[condition, 'col'] = value")
        lines.append("## MERGE (SAS merge) -> pd.merge(left, right, on=..., how=...)")
        lines.append("## RETAIN / LAG -> df['prev'] = df['col'].shift(1)")
        if has_loop:
            lines.append("## DO loops que generan filas -> construir lista de dicts y pd.DataFrame(rows) luego pd.concat")
        lines.append(f"# Guardar resultado en Postgres: df.to_sql('{dest.split('.')[-1]}', con, schema='{dest.split('.')[0]}' if '.' in '{dest}' else None, if_exists='replace', index=False)")
        return "\n".join(lines)

    def build_report_and_sql(self, lines_per_hour=1000.0, efficiency=1.0):
        # cálculo de estimación simple (similar al anterior)
        complexity = 1.0 + 0.2 * self.proc_sql_count + 0.5 * self.complex_data_count
        effective_lines = max(1, self.total_lines) * complexity
        hours = effective_lines / (lines_per_hour * efficiency)
        td = timedelta(hours=hours)
        # construir SQL/report text
        parts = []
        parts.append("-- TRADUCCIÓN SAS -> POSTGRESQL (Jupyter versión mejorada)\n")
        parts.append(f"-- Resumen: líneas={self.total_lines}, proc_sql={self.proc_sql_count}, data={self.data_count}, data_complejos={self.complex_data_count}")
        parts.append("\n-- MACROS RESUELTAS:")
        for k,v in self.macros.items():
            parts.append(f"--   {k} = {v}")
        parts.append("\n-- LIBRERÍAS LEÍDAS (libname):")
        for k,v in self.libraries.items():
            parts.append(f"--   {k} -> {v}")
        parts.append("\n-- TABLAS / ARCHIVOS DE ENTRADA DETECTADOS:")
        for t in sorted(self.inputs):
            parts.append(f"--   {t}")
        parts.append("\n-- TABLAS / ARCHIVOS DE SALIDA DETECTADOS:")
        for t in sorted(self.outputs):
            parts.append(f"--   {t}")
        if self.warnings:
            parts.append("\n-- ADVERTENCIAS / ITEMS A REVISAR:")
            for w in self.warnings:
                parts.append(f"-- * {w}")
        parts.append("\n-- BLOQUES TRADUCIDOS Y SUGERENCIAS:\n")
        for b in self.blocks:
            parts.append(b['comment'])
            # agregar PG propuesto si existe
            if b.get('pg'):
                parts.append("-- SQL sugerido (PG):")
                parts.append(b['pg'])
            parts.append("\n-- --------------------------------------------------\n")

        parts.append("-- ESTIMACIÓN DE MIGRACIÓN (heurística):")
        parts.append(f"--   complexity_factor = {complexity:.2f}")
        parts.append(f"--   effective_lines = {effective_lines:.0f}")
        parts.append(f"--   estimated_hours = {hours:.2f} h  (~ {td})")
        text = "\n".join(parts)
        summary = {
            'total_lines': self.total_lines,
            'proc_sql_count': self.proc_sql_count,
            'data_count': self.data_count,
            'complex_data_count': self.complex_data_count,
            'macros': self.macros,
            'libraries': self.libraries,
            'inputs': sorted(list(self.inputs)),
            'outputs': sorted(list(self.outputs)),
            'warnings': self.warnings,
            'estimation': {
                'complexity_factor': complexity,
                'effective_lines': effective_lines,
                'estimated_hours': hours,
                'estimated_timedelta': str(td),
                'lines_per_hour': lines_per_hour,
                'efficiency': efficiency
            }
        }
        return text, summary

# ---------------- Función principal para Jupyter ----------------
def sas_to_pg_jupyter(input_path="ejemplo.sas",
                      output_sql="ejemplo_convertido.sql",
                      output_json="ejemplo_reporte.json",
                      lines_per_hour=1200.0,
                      efficiency=0.9):
    p = Path(input_path)
    if not p.exists():
        raise FileNotFoundError(f"No existe el archivo {input_path}")
    text = p.read_text(encoding='utf-8', errors='ignore')
    conv = SAS2PG_Jupyter(text, base_path=p.parent)
    conv.preprocess()
    conv.translate()
    sql_text, summary = conv.build_report_and_sql(lines_per_hour=lines_per_hour, efficiency=efficiency)
    Path(output_sql).write_text(sql_text, encoding='utf-8')
    Path(output_json).write_text(json.dumps(summary, indent=2, ensure_ascii=False), encoding='utf-8')
    print(f"Generado: {output_sql}  y  {output_json}")
    return summary

